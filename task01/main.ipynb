{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Donwload and parse data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-09-28 02:15:20--  http://mit.spbau.ru/sewiki/images/8/87/Wikipedia_2000_dump.xml.gz\n",
      "Resolving mit.spbau.ru... 194.85.238.20\n",
      "Connecting to mit.spbau.ru|194.85.238.20|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13902445 (13M) [application/x-gzip]\n",
      "Saving to: ‘Wikipedia_2000_dump.xml.gz’\n",
      "\n",
      "Wikipedia_2000_dump 100%[===================>]  13.26M  4.28MB/s    in 3.1s    \n",
      "\n",
      "2017-09-28 02:15:23 (4.28 MB/s) - ‘Wikipedia_2000_dump.xml.gz’ saved [13902445/13902445]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd data && ./download.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE=\"data/Wikipedia_2000_dump.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def xml2df(xml_data):\n",
    "    root = ET.XML(xml_data)\n",
    "    all_records = []\n",
    "    for child in root:\n",
    "        record = {}\n",
    "        for name, value in child.attrib.items():\n",
    "            record[name] = value\n",
    "        record[\"content\"] = child.text\n",
    "        all_records.append(record)\n",
    "    return pd.DataFrame(all_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xml2df(open(DATA_FILE).read())[\"content\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Литва́ , официальное название\\xa0— Лито́вская Респу́блика \\xa0— государство, географически расположенное в Северной Европе (Прибалтика). Столица страны\\xa0— Вильнюс.\\n\\nРасположена на восточном побережье Балтийского моря. На севере граничит с Латвией, на юго-востоке\\xa0— с Белоруссией, на юго-западе\\xa0— c Польшей и Калининградской областью России.\\n\\nЧлен ООН с 1991 года, ЕС и НАТО\\xa0— с 2004 года. Входит в Шенгенскую зону и Еврозону.\\n\\nПоверхность\\xa0— равнинная со следами древнего оледенения. Поля и луга занимают 57\\xa0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "Preprocess data, then vectorize it using simple BOW model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk import sent_tokenize, wordpunct_tokenize\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class Preprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._punct = set(string.punctuation + \"«»№\")\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def _filter_gen(self, text):\n",
    "        text = \"\".join(filter(lambda c: c != '́', text))\n",
    "        for sent in sent_tokenize(text):\n",
    "            for word in wordpunct_tokenize(sent):\n",
    "                if word.isalpha():\n",
    "                    yield word.lower()\n",
    "    \n",
    "    def _tokenize(self, text):\n",
    "        return list(self._filter_gen(text))\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return list(\" \".join(self._tokenize(text)) for text in X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Литва́ , официальное название\\xa0— Лито́вская Респу́блика \\xa0— государство, географически расположенное в Северной Европе (Прибалтика). Столица страны\\xa0— Вильнюс.\\n\\nРасположена на восточном побережье Балтийского моря. На севере граничит с Латвией, на юго-востоке\\xa0— с Белоруссией, на юго-западе\\xa0— c Польшей и Калининградской областью России.\\n\\nЧлен ООН с 1991 года, ЕС и НАТО\\xa0— с 2004 года. Входит в Шенгенскую зону и Еврозону.\\n\\nПоверхность\\xa0— равнинная со следами древнего оледенения. Поля и луга занимают 57\\xa0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor()\n",
    "data[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'литва официальное название литовская республика государство географически расположенное в северной европе прибалтика столица страны вильнюс расположена на восточном побережье балтийского моря на севере граничит с латвией на юго востоке с белоруссией на юго западе c польшей и калининградской областью россии член оон с года ес и нато с года входит в шенгенскую зону и еврозону поверхность равнинная со следами древнего оледенения поля и луга занимают'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.transform([data[0][:500]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer as BagOfWords\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "model = make_pipeline(\n",
    "    Preprocessor(),\n",
    "    BagOfWords()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 283507)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = model.fit_transform(data)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x283507 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1788050 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction\n",
    "\n",
    "Calculate erank, then do the reduction using LSA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.sparse import linalg\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def erank(M):\n",
    "    u = linalg.svds(M.astype(float), k=min(M.shape) - 1, return_singular_vectors=False)\n",
    "    return math.exp(stats.entropy(u / sum(u)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = erank(X)\n",
    "m = int(round(e))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1049)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "\n",
    "X_reduced = TruncatedSVD(n_components=m, algorithm=\"arpack\").fit_transform(X.astype(float))\n",
    "X_reduced.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
